# AWS Capstone Project â€“ ETL Data Pipeline with Terraform

This project is a fully automated **ETL Data Pipeline on AWS**, built using **Terraform** for Infrastructure-as-Code. It demonstrates how to ingest, transform, and report on e-commerce data using modern cloud-native tools.

---

## ğŸš€ Project Overview
The pipeline is designed to:
1. **Ingest Data** â€“ Data is uploaded to an S3 bucket.  
2. **Trigger Processing** â€“ An AWS Lambda function is invoked automatically.  
3. **Orchestrate Workflow** â€“ The Lambda triggers an AWS Step Function to coordinate tasks.  
4. **Transform Data** â€“ AWS Glue performs cleaning, joins, and transformations.  
5. **Catalog & Query** â€“ A Glue Crawler updates the Data Catalog, enabling queries in Athena.  
6. **Generate Reports** â€“ Lambda functions create summary reports (by category, by payment method, counts) and store them back in S3.  

This project highlights **event-driven architecture, ETL orchestration, serverless compute, and Infrastructure-as-Code**.

---

## ğŸ—ï¸ Architecture
- **AWS S3** â€“ Data lake storage for raw and processed data.  
- **AWS Lambda** â€“ Custom ETL/reporting functions written in Python.  
- **AWS Glue** â€“ Transformation jobs for scalable data processing.  
- **AWS Step Functions** â€“ Orchestration of ETL workflows.  
- **AWS Glue Crawler** â€“ Automated schema discovery for Athena queries.  
- **Athena** â€“ SQL-based querying on curated datasets.  
- **IAM** â€“ Fine-grained access control.  
- **Terraform** â€“ End-to-end infrastructure automation.

---

## ğŸ“‚ Repository Structure
